import requests
import json
import csv
import time
import os
import re 
from datetime import datetime, timedelta

# --- é…ç½® (å·²æ›´æ–°) ---
FLINK_HOST = "localhost"
FLINK_PORT = 18081
BASE_URL = f"http://{FLINK_HOST}:{FLINK_PORT}"
METRICS_CSV_FILE = "metrics.csv"
STATUS_CSV_FILE = "status.csv"
RESTART_CSV_FILE = "restart.csv"      
POLL_INTERVAL_SECONDS = 1  # è½®è¯¢é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰

# --- æŒ‡æ ‡é…ç½®ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯ç»„åˆè¿›è¡Œæ¨¡ç³ŠåŒ¹é… ---
LATENCY_KEYWORD_A = "fetch-latency-avg" 
THROUGHPUT_KEYWORDS = ["numRecordsOutPerSecond", "Source"] 

# --- å…¨å±€å˜é‡ ---
JOB_ID = None           
SOURCE_VERTEX_ID = None 
metric_id_cache = {} 
restarting_start_time = None
is_restarting = False
last_status = None

# -----------------------------------------------------------
# ğŸ› ï¸ è‡ªåŠ¨å‘ç°å‡½æ•° (ç•¥)
# -----------------------------------------------------------

def discover_job_and_source_ids():
    """è‡ªåŠ¨å‘ç°å¤„äº RUNNING çŠ¶æ€çš„ JOB_ID å’Œå…¶ Source ç®—å­çš„ IDã€‚"""
    # ... (ä»£ç ä¸ä¸Šä¸€ä¸ªç‰ˆæœ¬ç›¸åŒï¼Œä¸ºç®€æ´æ­¤å¤„çœç•¥)
    global JOB_ID, SOURCE_VERTEX_ID
    
    jobs_url = f"{BASE_URL}/jobs"
    try:
        print("ğŸ” å°è¯•è·å–æ­£åœ¨è¿è¡Œçš„ Flink Job ID...")
        response = requests.get(jobs_url, timeout=5)
        response.raise_for_status()
        jobs_data = response.json()
        
        running_job = next((
            job for job in jobs_data.get('jobs', []) if job.get('status') == 'RUNNING'
        ), None)
        
        if running_job:
            JOB_ID = running_job['id']
            print(f"âœ… æˆåŠŸå‘ç° JOB ID: {JOB_ID} (çŠ¶æ€: RUNNING)")
        else:
            print("âŒ æœªæ‰¾åˆ°å¤„äº 'RUNNING' çŠ¶æ€çš„ Jobã€‚è¯·å¯åŠ¨ä¸€ä¸ª Job æˆ–æ‰‹åŠ¨è®¾ç½® JOB_IDã€‚")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Flink Job Manager æˆ–è·å– Job åˆ—è¡¨: {e}")
        return False
        
    job_details_url = f"{BASE_URL}/jobs/{JOB_ID}"
    try:
        print(f"ğŸ” å°è¯•è·å– Job {JOB_ID} çš„ Source Vertex ID...")
        response = requests.get(job_details_url, timeout=5)
        response.raise_for_status()
        job_details = response.json()
        
        source_vertex = next((
            vertex for vertex in job_details.get('vertices', []) 
            if 'Source' in vertex.get('name', '')
        ), None)
        
        if source_vertex:
            SOURCE_VERTEX_ID = source_vertex['id']
            print(f"âœ… æˆåŠŸå‘ç° Source Vertex ID: {SOURCE_VERTEX_ID} (åç§°: {source_vertex['name']})")
            return True
        else:
            print("âŒ æœªèƒ½æ‰¾åˆ°åç§°ä¸­åŒ…å« 'Source' çš„ Vertex IDã€‚è¯·æ£€æŸ¥ Job å›¾ã€‚")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ æ— æ³•è·å– Job {JOB_ID} è¯¦æƒ…: {e}")
        return False
# -----------------------------------------------------------


# -----------------------------------------------------------
# ğŸ“Š Metrics CSV å‡½æ•° (ç•¥)
# -----------------------------------------------------------

def initialize_metrics_csv():
    """åˆå§‹åŒ– metrics.csv æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ã€‚"""
    fieldnames = ["timestamp", "Latency", "Throughput"]
    if not os.path.exists(METRICS_CSV_FILE) or os.stat(METRICS_CSV_FILE).st_size == 0:
        with open(METRICS_CSV_FILE, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
        print(f"âœ… åˆ›å»º/åˆå§‹åŒ– {METRICS_CSV_FILE} æ–‡ä»¶ã€‚")

def write_metrics_to_csv(timestamp, latency, throughput):
    """å°†æŒ‡æ ‡å†™å…¥ metrics.csv æ–‡ä»¶ã€‚"""
    fieldnames = ["timestamp", "Latency", "Throughput"]
    data = {
        "timestamp": timestamp,
        "Latency": latency,
        "Throughput": throughput
    }
    
    for key, value in data.items():
        if value is None:
            data[key] = "N/A"
            
    try:
        with open(METRICS_CSV_FILE, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(data)
    except IOError as e:
        print(f"âŒ å†™å…¥ CSV æ–‡ä»¶å¤±è´¥: {e}")

# -----------------------------------------------------------
# ğŸ†• æ–°å¢ï¼šçŠ¶æ€å’Œé‡å¯æ—¶é—´ CSV å‡½æ•°
# -----------------------------------------------------------

def initialize_status_csv():
    """åˆå§‹åŒ– status.csv æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ã€‚"""
    fieldnames = ["record_timestamp", "job_status"]
    if not os.path.exists(STATUS_CSV_FILE) or os.stat(STATUS_CSV_FILE).st_size == 0:
        with open(STATUS_CSV_FILE, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
        print(f"âœ… åˆ›å»º/åˆå§‹åŒ– {STATUS_CSV_FILE} æ–‡ä»¶ã€‚")

# ğŸŒŸ ä¿®æ”¹ç‚¹ 1: ä¿®æ”¹è¡¨å¤´ä»¥è®°å½•æ—¶é•¿ (Duration)
def initialize_restart_csv():
    """åˆå§‹åŒ– restart.csv æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ã€‚"""
    # æ›´æ”¹è¡¨å¤´åç§°ï¼Œä½¿å…¶æ›´å…·æè¿°æ€§
    fieldnames = ["record_timestamp", "duration_minus_60s_seconds"] 
    if not os.path.exists(RESTART_CSV_FILE) or os.stat(RESTART_CSV_FILE).st_size == 0:
        with open(RESTART_CSV_FILE, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
        print(f"âœ… åˆ›å»º/åˆå§‹åŒ– {RESTART_CSV_FILE} æ–‡ä»¶ã€‚")

def write_status_to_csv(record_timestamp, job_status):
    """å°† Job çŠ¶æ€å†™å…¥ status.csv æ–‡ä»¶ã€‚"""
    fieldnames = ["record_timestamp", "job_status"]
    data = {
        "record_timestamp": record_timestamp,
        "job_status": job_status if job_status is not None else "N/A (API Failed)"
    }
    try:
        with open(STATUS_CSV_FILE, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(data)
    except IOError as e:
        print(f"âŒ å†™å…¥ {STATUS_CSV_FILE} æ–‡ä»¶å¤±è´¥: {e}")

# ğŸŒŸ ä¿®æ”¹ç‚¹ 2: ä¿®æ”¹å‡½æ•°ä»¥è®¡ç®—å¹¶è®°å½• (restart_duration - 60)
def write_duration_minus_60_to_csv(record_timestamp, restart_duration):
    """
    è®¡ç®— 'restart_duration - 60s' å¹¶å°†ç»“æœå†™å…¥ restart.csv æ–‡ä»¶ã€‚
    restart_duration æ˜¯æµ®ç‚¹æ•°ï¼Œå•ä½ä¸ºç§’ã€‚
    """
    fieldnames = ["record_timestamp", "duration_minus_60s_seconds"]
    
    # è®¡ç®—æŒç»­æ—¶é—´å‡å» 60 ç§’
    calculated_duration = restart_duration - 60.0
    
    data = {
        # è®°å½• Job çŠ¶æ€å˜ä¸º RUNNING æ—¶çš„å½“å‰æ—¶é—´æˆ³
        "record_timestamp": record_timestamp, 
        # è®°å½•è®¡ç®—åçš„æ—¶é•¿
        "duration_minus_60s_seconds": f"{calculated_duration:.4f}" 
    }
    try:
        with open(RESTART_CSV_FILE, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(data)
        print(f"ğŸ“ Job é‡å¯æ—¶é•¿ï¼ˆ-60sï¼‰å·²è®°å½•åˆ° {RESTART_CSV_FILE}ï¼š{data['duration_minus_60s_seconds']} ç§’")
    except IOError as e:
        print(f"âŒ å†™å…¥ {RESTART_CSV_FILE} æ–‡ä»¶å¤±è´¥: {e}")


# -----------------------------------------------------------
# ğŸ¯ æ ¸å¿ƒå‡½æ•° (ç•¥)
# -----------------------------------------------------------
def get_job_metrics():
    # ... (ä»£ç ä¸ä¸Šä¸€ä¸ªç‰ˆæœ¬ç›¸åŒï¼Œä¸ºç®€æ´æ­¤å¤„çœç•¥)
    global metric_id_cache
    
    metrics_config = {
        'Latency': {'keywords': [LATENCY_KEYWORD_A]},
        'Throughput': {'keywords': THROUGHPUT_KEYWORDS}
    }
    
    subtask_index = 0
    metrics_url_all = (
        f"{BASE_URL}/jobs/{JOB_ID}/vertices/{SOURCE_VERTEX_ID}/subtasks/{subtask_index}/metrics"
    )

    try:
        if len(metric_id_cache) < 2:
            response = requests.get(metrics_url_all, timeout=5)
            response.raise_for_status()
            all_metrics_data = response.json()
            metric_id_cache.clear()
            for key, config in metrics_config.items():
                match = next((
                    item['id'] for item in all_metrics_data 
                    if all(keyword in item['id'] for keyword in config['keywords'])
                ), None)
                if not match: return None
                metric_id_cache[key] = match

        if not metric_id_cache: return None

        metric_ids_for_query = list(metric_id_cache.values())
        metrics_url_specific = (
             f"{metrics_url_all}?get={','.join(metric_ids_for_query)}"
        )
        
        response = requests.get(metrics_url_specific, timeout=5)
        response.raise_for_status()
        metrics_data = response.json()
        
        metrics_map = {item['id']: item.get('value') for item in metrics_data}
        
        return {
            "Latency": metrics_map.get(metric_id_cache.get('Latency')),
            "Throughput": metrics_map.get(metric_id_cache.get('Throughput'))
        }
        
    except requests.exceptions.RequestException as e:
        return None

def get_job_status():
    if not JOB_ID: return None 
    try:
        response = requests.get(f"{BASE_URL}/jobs/{JOB_ID}", timeout=5)
        response.raise_for_status()
        data = response.json()
        return data.get('state')
    except requests.exceptions.RequestException as e:
        return None
# -----------------------------------------------------------


# -----------------------------------------------------------
# --- ä¸»å¾ªç¯ (æ›´æ–°äº†é‡å¯æ—¶é•¿è®°å½•é€»è¾‘) ---
# -----------------------------------------------------------

def main():
    global restarting_start_time, is_restarting, last_status, JOB_ID, SOURCE_VERTEX_ID
    
    if not discover_job_and_source_ids():
        print("\nâš ï¸ æ— æ³•å¯åŠ¨ç›‘æ§ï¼šæœªæˆåŠŸå‘ç°æ‰€éœ€çš„ Job æˆ– Source IDã€‚")
        return

    # åˆå§‹åŒ–æ‰€æœ‰ CSV æ–‡ä»¶
    initialize_metrics_csv()
    initialize_status_csv()   
    initialize_restart_csv()  
    
    print(f"\nğŸš€ å¼€å§‹ç›‘æ§ Flink Job (ID: {JOB_ID})ï¼Œè½®è¯¢é—´éš” {POLL_INTERVAL_SECONDS} ç§’...")
    print(f"ğŸ¯ Source Vertex ID: {SOURCE_VERTEX_ID}")
    print(f"æ•°æ®å°†åˆ†åˆ«å†™å…¥ {METRICS_CSV_FILE}, {STATUS_CSV_FILE}, {RESTART_CSV_FILE}\n")

    try:
        while True:
            current_time = datetime.now()
            # ä½¿ç”¨å¾®ç§’ç²¾åº¦çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œç”¨äºæ‰€æœ‰è®°å½•
            current_time_str = current_time.strftime("%Y-%m-%d %H:%M:%S.%f")
            
            current_status = get_job_status()
            
            # --- çŠ¶æ€æ£€æŸ¥å’Œé‡å¯æ—¶é—´è®¡ç®— ---
            
            if current_status:
                
                # è®°å½•çŠ¶æ€åˆ° status.csv
                write_status_to_csv(current_time_str, current_status)
                
                if current_status == 'RESTARTING' and not is_restarting:
                    is_restarting = True
                    restarting_start_time = time.time()
                    print(f"ğŸ”¥ [{current_time.strftime('%H:%M:%S')}] Job çŠ¶æ€å˜ä¸º **RESTARTING**ã€‚å¼€å§‹è®¡æ—¶...")
                
                elif current_status == 'RUNNING' and is_restarting:
                    is_restarting = False
                    restarting_end_time = time.time()
                    restart_duration = restarting_end_time - restarting_start_time
                    
                    # ğŸŒŸ ä¿®æ”¹ç‚¹ 3: è®°å½• restart_duration - 60s
                    write_duration_minus_60_to_csv(current_time_str, restart_duration)
                    
                    print(f"âœ… [{current_time.strftime('%H:%M:%S')}] Job çŠ¶æ€å˜å› **RUNNING**ã€‚")
                    print(f"â±ï¸ Job é‡å¯æŒç»­æ—¶é—´: **{restart_duration:.4f} ç§’**\n")
                    restarting_start_time = None
                    
                if current_status != last_status:
                    print(f"â„¹ï¸ [{current_time.strftime('%H:%M:%S')}] Job çŠ¶æ€: **{current_status}**")
                
                last_status = current_status
            else:
                # è®°å½•çŠ¶æ€ä¸º N/A åˆ° status.csv
                write_status_to_csv(current_time_str, None)
                print(f"âš ï¸ [{current_time.strftime('%H:%M:%S')}] æ— æ³•è·å– Job çŠ¶æ€ã€‚è¯·æ£€æŸ¥ Job Manager æ˜¯å¦è¿è¡Œã€‚")

            # --- æŒ‡æ ‡æ”¶é›† ---

            metrics = get_job_metrics()
            
            latency = None
            throughput = None
            
            if metrics:
                latency = metrics.get('Latency')
                throughput = metrics.get('Throughput')
                
                if latency is not None and throughput is not None:
                     print(f"ğŸ“Š [{current_time.strftime('%H:%M:%S')}] Latency: {latency}, Throughput: {throughput}")
            
            # å†™å…¥ metrics.csv (ä½¿ç”¨å¾®ç§’ç²¾åº¦æ—¶é—´æˆ³)
            write_metrics_to_csv(
                timestamp=current_time_str,
                latency=latency,
                throughput=throughput
            )
            
            time.sleep(POLL_INTERVAL_SECONDS)

    except KeyboardInterrupt:
        print("\n gracefully é€€å‡ºç›‘æ§è„šæœ¬ã€‚")
    except Exception as e:
        print(f"\nå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

if __name__ == "__main__":
    main()