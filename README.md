## 研究目的
探究Flink检查点间隔对性能与容错能力的影响

## 研究内容
深入理解Flink的检查点（Checkpoint）机制，探究检查点间隔对系统性能和故障恢复能力的影响。进一步分析不同间隔设置下的性能权衡，研究如何选择合适的检查点间隔以在性能与
容错之间取得平衡。

## 实验

### 实验环境
| 设备名称     | CPU核心数 | 内存总量  | 网络带宽数    | 存储类型 | 操作系统         |
| -------- | ------ | ----- | -------- | ---- | ------------ |
| master   | 64     | 754GB | 1000Mb/s | HDD  | Ubuntu 20.04 |
| slave-01 | 96     | 1.0TB | 1000Mb/s | HDD  | Ubuntu 20.04 |
| slave-02 | 64     | 754GB | 1000Mb/s | HDD  | Ubuntu 20.04 |
| slave-03 | 64     | 754GB | 1000Mb/s | HDD  | CentOS 7     |
| slave-04 | 64     | 754GB | 1000Mb/s | HDD  | Ubuntu 20.04 |


> JDK 版本：JDK 17
> 
> kafka版本：Kafka 7.4.0
> 
> Flink版本：Flink 1.19.3
> 
> MiniO：2025-09-07T16-13-09Z

### 实验负载
##### 数据集
本次实验选择的数据集是**纽约出租车数据集**，纽约出租车数据是一个非常经典且广泛使用的公开数据集，来源于纽约市出租车与豪华轿车委员会(TLC)的数据发布。

该数据集中我们截取了2023年1月1日-2023年6月30日这个时间段内一共19493547条数据，其中用到的字段有：

| 字段名称             | 含义                | 用途                                                  |
|------------------|-------------------|-----------------------------------------------------|
| `pickupDatetime` | 上车时间 | 窗口聚合（2小时滚动窗口）的时间基准，转换为 UTC 毫秒级时间戳                   |
| `puLocationID`   | 上车区域 ID     | 按上车区域划分数据，用于窗口聚合                                    |
| `totalAmount`    | 收入金额              | 在窗口聚合中计算总收入（`TotalIncome = Σ totalAmount`），作为核心聚合指标 |

使用该数据集可以兼备**计算密集**、**状态密集**、**数据密集**三个特点

关于数据处理方面，我们在原有数据的基础上处理了乱序数据，让所有数据按照时间序列排序，同时筛选掉早于 2023-01-01 的行数据以及存在关键字段缺失值的数据，可以参考[sort_nyc_csv.py](code/flink-checkpoint-experiment/sort_nyc_csv.py)

##### 工作负载
我们使用脚本读取 NYC出租车CSV文件，然后按时间顺序把每一行发送到`Kafka`，并且以7200倍的时间倍率(即现实生活中1s=数据集中时间7200s)提高每秒钟发送的数据数量

我们的实验根据时间戳做滚动窗口，按`PULocationID`维护每个位置的**累计收入**和**行程数量**,同时使用脚本访问REST API 获取`AvgLatency(ms)`、`Throughput(rec/s)`、`CheckpointDuration(ms)`等指标用于后续分析

本次实验控制使用的checkpoint间隔分别为：30s、2min、5min、10min，观察不同checkpoint间隔下系统延迟、吞吐量、检查点持续时间及故障恢复时间的变化

### 实验步骤
1. 将原始数据集放入`./flink-checkpoint-experiment/docker/assets/datasets/nyc-taxi`下，运行`sort_nyc_csv.py`,得到`nyc-taxi-2023-01-fixed.csv`
2. 运行`master-start.sh`，`slaves-start.sh`，启动`JobManager`和`TaskManager`
3. 运行`serv_nyc_csv_kafka.py`，发送数据到`kafka`
4. 在浏览器中输入localhost:18081,submit jar中提交任务(在experiment中运行`mvn clean package`获得jar)
5. 运行`monitor.py`,`fault.sh`记录数据
6. `curl http://local host:18081/jobs/<job-id>/checkpoints`获取checkpoint日志内容

### 实验结果与分析

通过实验结果，可以看到系统在恢复初期、积压处理和追赶实时三个阶段，性能表现不同

| **阶段**       | **现象**                                   | **机制解释**                                                                                 | **吞吐量**                | **延迟** | **变化关联逻辑**                                                                 |
|----------------|--------------------------------------------|------------------------------------------------------------------------------------------|---------------------------|------------------------------------------------|----------------------------------------------------------------------------------|
| **I. 恢复初期** | 任务重启、加载状态、Source 重置 Offset。   | 所有计算资源用于**恢复状态和配置**，数据处理停滞；部分 Task 未启动，上下游链路未打通。                                         | **极低或为零**（变小）    | **显著升高**（变大）                  | 资源优先分配给状态恢复，数据处理停滞→吞吐量趋近于0；Source 无有效拉取/算子未就绪→延迟异常或无数据。 |
| **II. 积压处理** | Flink 开始高速处理积压数据。               | Source 从 Kafka 快速拉取积压数据，所有算子以**最大并行度和峰值性能**处理；系统可能出现轻微资源竞争（网络 I/O、CPU 满负荷），但核心瓶颈在数据处理速率。 | **激增，显著高于稳定态**（变大） | **短暂升高后逐步下降**（先升后降）              | 吞吐量激增是因系统全力追赶积压数据；延迟先升是因 Kafka 拉取压力/算子缓冲区排队，后降是因积压数据减少、资源竞争缓解。 |
| **III. 追赶实时** | 积压数据处理完毕，系统接近实时流。         | 处理速度与 Kafka 实时输入速率匹配；算子负载稳定，数据无堆积，Source 拉取延迟恢复正常。                                       | **回落到稳定态**（回落）  | **降至稳定值并保持平稳**（回落至基准）          | 吞吐量与实时数据输入速率平衡；Source 拉取无排队、算子无数据堆积→延迟稳定在“网络传输+算子处理”基准值。 |


#### 1. 检查点持续时间
- 30秒：检查点触发频率过高，系统需持续执行状态快照操作。虽单次检查点持续时间最短（快照数据量较小），但频繁的磁盘I/O与网络传输导致**总开销占比极高**，对系统资源形成持续占用。

- 2分钟：检查点间隔设置适中，单次快照的状态数据量较30秒间隔有所增加，因此持续时间更长，但因触发频率降低，系统整体开销趋于平衡。

- 5分钟：为实验中的**最佳平衡点**。尽管单次检查点状态量进一步增大，但合理的触发频率使系统有充足时间处理业务数据，避免被频繁的快照操作干扰，兼顾单次耗时与触发频率。

- 10分钟：检查点间隔过长，单次快照需处理的状态数据量达到最大值，导致**持续时间最长**；虽触发频率低使总开销可能较低，但故障恢复时需处理的历史数据量显著增加。

结论：检查点间隔与单次持续时间呈正相关，间隔越长，单次耗时越长，但总开销呈非线性变化。5分钟间隔在单次快照耗时与触发频率之间实现了最优平衡。



#### 2. 系统吞吐量

- 30秒：频繁的检查点操作占用大量CPU、I/O及网络资源，严重挤占业务数据处理资源，导致**吞吐量最低**。

- 2分钟：检查点对资源的挤占效应减弱，吞吐量较30秒间隔明显提升，但仍受周期性快照操作干扰。

- 5分钟：检查点干扰降至最低，系统可将主要资源用于业务处理，吞吐量达到**峰值水平**。

- 10分钟：吞吐量略高于5分钟间隔，但提升幅度微弱。此时检查点开销已非系统瓶颈，集群硬件资源限制等其他因素成为吞吐量提升的主要约束。

结论：吞吐量随检查点间隔增加呈上升趋势，在5分钟间隔后趋于稳定，5分钟是吞吐量提升的关键拐点。



#### 3. 处理延迟

- 30秒：频繁的检查点易引发系统背压（backpressure），数据处理流程频繁被快照操作中断，导致**延迟最高**。

- 2分钟：检查点对数据处理的中断频率降低，延迟较30秒间隔明显下降。

- 5分钟：系统运行稳定性最优，检查点对业务的干扰最小，延迟达到**最低水平**。

- 10分钟：稳态延迟与5分钟间隔基本持平，但故障恢复阶段的延迟显著增加。

结论：延迟随检查点间隔增加呈下降趋势，5分钟间隔后延迟趋于稳定，达到系统最优基准水平。



#### 4. 故障恢复时间

- 30秒：最近一次检查点时间点距离故障发生时间最近，状态数据丢失量最少，因此**恢复时间最短**。

- 2分钟：需恢复的状态数据量较30秒间隔增加，恢复时间相应延长。

- 5分钟：恢复时间进一步增加，但仍处于业务可接受的合理范围内。

- 10分钟：需从最早的检查点恢复，状态重放与数据补算量最大，导致**恢复时间最长**。

结论：故障恢复时间与检查点间隔呈严格正相关，间隔越长，恢复所需时间越长。

#### 5. **资源利用率**

- 短间隔（30秒、2分钟）：大量资源被消耗在检查点的快照生成、数据传输与存储过程中，资源利用率偏低，存在明显浪费。
- 5分钟：资源在业务处理与检查点操作之间分配均衡，利用率曲线更平稳，整体效率最高。
- 10分钟：稳态资源利用率较高，但过长的检查点间隔导致故障恢复风险陡增，可用性下降。



### 总结：为什么5分钟是本实验的最佳间隔？

1. 性能平衡最优：5分钟间隔在吞吐量与延迟两大核心指标上实现了最佳平衡——既避免了短间隔（30秒）下频繁检查点的资源挤占问题，又未因间隔过长（10分钟）导致状态管理压力增大，系统运行稳定性最优。
2. 容错性合理可控：5分钟间隔对应的故障恢复时间虽长于30秒间隔，但仍处于业务可接受范围，同时规避了10分钟间隔下恢复时间过长的风险，在容错能力与系统开销之间取得平衡。
3. 适配数据集特性：本次实验采用的纽约出租车数据集（1949万条记录，覆盖6个月）状态增长速率适中，5分钟间隔使每次检查点的状态数据量处于集群处理能力的最优区间，避免了状态过大导致的快照失败风险。

### 建议的检查点间隔设置策略：

1. 低延迟优先场景：若业务对故障恢复时间要求严苛（如金融实时风控），可选择1-3分钟的较短间隔，但需权衡接受一定的吞吐量损失。
2. 高吞吐优先场景：若业务对延迟敏感度较低、追求极致吞吐量（如离线数据补算），可选择5-10分钟的较长间隔，但需确保故障恢复时间在可接受范围内。
3. 动态调整策略：未来可探索基于实时监控数据的动态调整机制，根据状态大小变化、数据流速波动、集群负载情况自适应调整检查点间隔，实现更精细化的资源管理。
4. **根据公式**：
* **参数**：
  * **T**：检查点间隔
  * **c**：检查点成本
  * **R**：重启成本
  * **λ**：系统故障率
  * **n**：应用深度（算子个数）
  * **δ**：检查点令牌传递延迟

* 最优检查点间隔 **T** 通过最大化利用率 **U** 求得，公式为：
> $$
  T=\frac{cλ+W(−e^{−cλ−1})+1}{λ}
  $$
  
  其中**W** 是**Lambert W函数**，用于求解隐式方程。



### 实验结论：

实验证实，检查点间隔对Flink系统性能存在显著影响。针对本次实验的纽约出租车数据集处理场景，**5分钟检查点间隔在吞吐量、延迟与故障恢复时间之间取得了最佳平衡**，为该场景下的最优配置。具体表现为：短间隔（30秒）因频繁快照导致吞吐量下降、延迟升高；长间隔（10分钟）虽略微提升稳态吞吐量，但大幅延长故障恢复时间，可用性降低。
因此，实际生产环境中需结合作业特性，综合考量**系统故障率、检查点成本、重启成本、应用结构**等核心因素，动态选择适配的检查点间隔，并通过持续监控与迭代调优实现性能与可用性的最优平衡。

### 分工
- 闭玉申：实验环境搭建、核心流程实现与集群部署
- 唐小卉：数据集选取与工作负载定义、核心指标采集方案设计、监控脚本开发、实验结果核心分析
- 余婧：实验数据可视化呈现、运行脚本性能优化、实验结果补充分析
- 仲韦萱：预实验验证与问题排查、实验报告PPT制作与成果汇报


## 补充：关于最佳时间的推导过程
在论文《Optimizing checkpoint-based fault-tolerance in distributed stream processing systems: Theory to practice》中，作者提出了一个用于**优化全局检查点间隔的数学模型**，目的是最大化分布式流处理系统的**系统利用率**。该系统利用率被定义为系统用于执行有用工作的时间占总时间的比例。


## 一、数学模型概述

模型基于以下几个关键参数：

- **λ**：系统故障率（failure rate）。
- **T**：检查点间隔（checkpoint interval）。
- **c**：执行一次检查点的成本（checkpoint cost），即检查点操作所需时间。
- **R**：重启成本（restart cost），包括故障检测、状态恢复时间。
- **δ**：检查点令牌在算子间传递的时间（token communication time）。
- **n**：流处理应用的深度（算子图中的路径长度）。

系统利用率 \( U \) 定义为：
$$
U = \frac{T - c}{T_{eff}}
$$
其中 $T_{eff}$ 是**考虑到故障和恢复后的有效执行时间**。

---

## 二、模型推导过程

### 1. 考虑故障与恢复的时间模型

系统在时间 \( T \) 内可能发生故障，每次故障后需要重启并重新处理从上次检查点以来的数据。因此，有效执行时间包括：


作者最终推导出**系统利用率 \( U \) 的表达式**：
$$
U = \frac{\lambda e^{\delta \lambda} (T - c)}{e^{\lambda(R + T + \delta n)} - e^{\lambda(R + \delta n)}}
$$
---

### 2. 最优检查点间隔 

为了最大化 \( U \)，对 \( U \) 关于 \( T \) 求导，令导数为零：
$$
\frac{\partial U}{\partial T} = 0
$$
解此方程得到**最优检查点间隔**的表达式：
$$
T^* = \frac{c \lambda + W(-\mathrm{e}^{-c \lambda - 1}) + 1}{\lambda}
$$
其中 W是朗伯W函数（Lambert W function)









